{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b4243b-ca14-43d3-b2bd-591edd096e0a",
   "metadata": {
    "id": "81b4243b-ca14-43d3-b2bd-591edd096e0a"
   },
   "source": [
    "# CS 5242 Assignment 8\n",
    "\n",
    "In this assignment, we are going to dive into autoencoders (AE) and variational autoencoders (VAE). AEs are powerful discriminative models while VAEs are widely used in generation tasks.\n",
    "\n",
    "### **Submission**\n",
    "\n",
    "ASSIGNMENT DEADLINE ‚è∞ : **23:59 09 Nov 2025**\n",
    "\n",
    "Rename this file as \"{StuID}_{Name}_assignment-5.ipynb\" (e.g., \"A0100000J_John-Doe_assignment-5.ipynb\"), and submit it to Canvas. Make sure all outputs are saved in this file as we will not run any code for you. Do **not** submit any other files, especially dataset files.\n",
    "\n",
    "\n",
    "### **Contact**\n",
    "\n",
    "Feel free to reach me if you have any question about this assignment.\n",
    "\n",
    "Slack: Yiqi Zhang\n",
    "\n",
    "Email: yiqi.zhang@u.nus.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbf757-1117-4f9e-97d5-05b1affcbd38",
   "metadata": {
    "id": "eccbf757-1117-4f9e-97d5-05b1affcbd38"
   },
   "source": [
    "## Task 1: Training an autoencoder (AE)\n",
    "In task 1, the goal is to train an autoencoder (AE), which consists of an encoder and a decoder. AE is capable of learning meaningful representations in the latent space, which could be used for tasks like classification. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96934b0-1651-4715-aa84-83690aa97807",
   "metadata": {
    "id": "b96934b0-1651-4715-aa84-83690aa97807"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(1)\n",
    "        x_enc = self.encoder(x)\n",
    "        x_dec = self.decoder(x_enc)\n",
    "        x_recon = x_dec.reshape(B, C, H, W)\n",
    "        return x_recon\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 32\n",
    "model = Autoencoder(input_dim, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf1f4a-c2dd-4fea-98f9-5de2a74294ea",
   "metadata": {
    "id": "e6bf1f4a-c2dd-4fea-98f9-5de2a74294ea"
   },
   "source": [
    "We have prepared the dataset and the AE model for you. Your task is to implement the training code and meet the following requirements:\n",
    "- use mean squared error (MSE) as the loss function\n",
    "- use a proper optimizer and a proper learning rate\n",
    "- train the model for 10 epochs\n",
    "- plot the training loss curve (at least 10 points in the curve, since we train 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2f132-e10d-4211-8259-cc6f6847a722",
   "metadata": {
    "id": "12d2f132-e10d-4211-8259-cc6f6847a722"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# TODO: train the AE model (2 points)\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2363660-e891-406a-860e-ffd18a965edd",
   "metadata": {
    "id": "f2363660-e891-406a-860e-ffd18a965edd"
   },
   "source": [
    "Verify your AE is well trained by visualizing reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e11bb8-5455-48b1-b8e1-461adb31f0d2",
   "metadata": {
    "id": "a4e11bb8-5455-48b1-b8e1-461adb31f0d2"
   },
   "outputs": [],
   "source": [
    "# Visualizing reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "sample_list = []\n",
    "for idx_class in range(10):\n",
    "    indices_i = (test_dataset.targets == idx_class).nonzero().view(-1)\n",
    "    idx_sample = indices_i[0]\n",
    "    sample = test_dataset[idx_sample]\n",
    "    sample_list.append(sample[0])\n",
    "x_orig = torch.stack(sample_list).to(device)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon = model(x_orig)\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "imgs_orig = make_grid(x_orig, nrow=10, padding=0, normalize=True)\n",
    "imgs_recon = make_grid(x_recon, nrow=10, padding=0, normalize=True)\n",
    "\n",
    "print('Input Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_orig.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Reconstructed Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_recon.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2c478-7a91-4fa9-ba24-07547fe22854",
   "metadata": {
    "id": "e8d2c478-7a91-4fa9-ba24-07547fe22854"
   },
   "source": [
    "Now we have trained an AE. Let's see how it can be used for classification. You are required to plot a [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) map of the latent representations output by the encoder. Implement the code below and follow these requirements:\n",
    "- plot a 2D t-SNE map\n",
    "- plot 20 samples for each class in the **test** set (i.e., 20 points for each digit from 0 to 9)\n",
    "- use different colors for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd0717-0a9e-41bb-a9f3-0d6e4b788d8c",
   "metadata": {
    "id": "d1cd0717-0a9e-41bb-a9f3-0d6e4b788d8c"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Plot t-SNE map (1.5 point)\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b316a6-dc39-4d1f-89b0-b41536e1bd30",
   "metadata": {
    "id": "f0b316a6-dc39-4d1f-89b0-b41536e1bd30"
   },
   "source": [
    "Can you see the separation/classification of different digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84593ae8-a343-4d62-a2ed-00be462d5163",
   "metadata": {
    "id": "84593ae8-a343-4d62-a2ed-00be462d5163"
   },
   "source": [
    "## Task 2: Training a variational autoencoder (VAE)\n",
    "We have trained an AE in task 1, which is demonstrated to be useful for classification. However, only the encoder of AE is used. The decoder part, which is capable of reconstruction, is wasted. Can we use the decoder for generating images?\n",
    "\n",
    "The answer is yes. But some modifications are needed to achieve this goal. The general idea is to impose a prior distribution $p(z)$ on the latent space and constrain the learned distribution $q(z|x)$ to be close to $p(z)$, so that we can gain control over the learned latent distribution. Then, we can generate images using the decoder by sampling data points from the latent distribution. Actually, this is all about what a variational autoencoder (VAE) could do.\n",
    "\n",
    "Typically, people use the standard normal distribution as the prior latent distribution (i.e., $p(z)= N(0, I)$), which has $ \\mu=0 $ and $ \\sigma=I $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86beb84-4439-4d09-921e-e7e76dad8355",
   "metadata": {
    "id": "d86beb84-4439-4d09-921e-e7e76dad8355"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu = nn.Linear(256, latent_dim)\n",
    "        self.log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        h = self.encoder(x)\n",
    "\n",
    "        mu, log_var = self.mu(h), self.log_var(h)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        x_dec = self.decoder(z)\n",
    "        x_recon = x_dec.reshape(B, C, H, W)\n",
    "\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 28 * 28\n",
    "latent_dim = 32\n",
    "model = VAE(input_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6ceb7",
   "metadata": {},
   "source": [
    "### Understanding the Reparameterization Trick (1.5 points)\n",
    "\n",
    "Before implementing the VAE training code, let's understand a critical component: the **reparameterization trick**.\n",
    "\n",
    "In the VAE model above, you'll notice the `reparameterize` function:\n",
    "```python\n",
    "def reparameterize(self, mu, log_var):\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "```\n",
    "\n",
    "**Question:** In the markdown cell below, answer the following:\n",
    "\n",
    "1. **What problem does the reparameterization trick solve?** Specifically, explain why we cannot directly sample from `z ~ N(mu, sigma^2)` during training if we want to use backpropagation.\n",
    "\n",
    "2. **How does the reparameterization trick work?** Explain the mathematical transformation that allows gradients to flow through the sampling operation.\n",
    "\n",
    "3. **What would happen if we didn't use this trick?** Describe the consequences for training the VAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a6c32",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**1. What problem does the reparameterization trick solve?**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**2. How does the reparameterization trick work?**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**3. What would happen without this trick?**\n",
    "\n",
    "[Your answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3ca7c-55b3-4730-b859-8611d020e209",
   "metadata": {
    "id": "a1c3ca7c-55b3-4730-b859-8611d020e209"
   },
   "source": [
    "Again, We have prepared the dataset and the VAE model for you. Your task is to implement the training code and meet the following requirements:\n",
    "- The loss function consists of 2 parts: 1) BinaryCrossEntropy (BCE) loss as the reconstruction loss; 2) KL divergence loss to minimize the distance between $q(z|x)$ and $p(z)$. Note that $p(z)= N(0, I)$\n",
    "- use a proper optimizer and a proper learning rate\n",
    "- train the model for 10 epochs\n",
    "- plot the training loss curve (at least 10 points in the curve, since we train 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ccc0e-7b9a-4bf3-a9d6-e5cfacd1c8f5",
   "metadata": {
    "id": "970ccc0e-7b9a-4bf3-a9d6-e5cfacd1c8f5"
   },
   "outputs": [],
   "source": [
    "# TODO: train the VAE model (3 points)\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4931-2fed-4257-a673-0cbbca345059",
   "metadata": {
    "id": "d12d4931-2fed-4257-a673-0cbbca345059"
   },
   "source": [
    "Also, verify the sanity of the VAE model by visualizing reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98f01e-8242-416a-b653-e7f10b577f22",
   "metadata": {
    "id": "7c98f01e-8242-416a-b653-e7f10b577f22"
   },
   "outputs": [],
   "source": [
    "# Visualizing reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "sample_list = []\n",
    "for idx_class in range(10):\n",
    "    indices_i = (test_dataset.targets == idx_class).nonzero().view(-1)\n",
    "    idx_sample = indices_i[0]\n",
    "    sample = test_dataset[idx_sample]\n",
    "    sample_list.append(sample[0])\n",
    "x_orig = torch.stack(sample_list).to(device)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon, mu, log_var = model(x_orig)\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "imgs_orig = make_grid(x_orig, nrow=10, padding=0, normalize=True)\n",
    "imgs_recon = make_grid(x_recon, nrow=10, padding=0, normalize=True)\n",
    "\n",
    "print('Input Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_orig.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Reconstructed Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_recon.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d1274-e498-4484-859c-4de01793cb34",
   "metadata": {
    "id": "c66d1274-e498-4484-859c-4de01793cb34"
   },
   "source": [
    "Now we have trained a VAE. Let's first check its latent distribution like we do for AE. You are required to plot a [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) map of the latent representations output by the VAE encoder. Implement the code below and follow these requirements:\n",
    "- plot a 2D t-SNE map\n",
    "- plot 20 samples for each class in the **test** set (i.e., 20 points for each digit from 0 to 9)\n",
    "- use different colors for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06f0b8-b2ac-4493-bf63-ded5208a4419",
   "metadata": {
    "id": "0c06f0b8-b2ac-4493-bf63-ded5208a4419"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Plot t-SNE map (1.5 point)\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142245a-8c7a-4e53-921b-35c2dddaa853",
   "metadata": {
    "id": "d142245a-8c7a-4e53-921b-35c2dddaa853"
   },
   "source": [
    "Finally, we are able to generate images using the decoder of the VAE by sampling data points from $p(z)$. Implement the code below and meet these requirements:\n",
    "- sample 10 data points from $p(z)$\n",
    "- show them 5 images in a row, 2 rows in total\n",
    "- at least one image should be recognized as a digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915d22e-7813-477b-86c5-1fd0697841a9",
   "metadata": {
    "id": "7915d22e-7813-477b-86c5-1fd0697841a9"
   },
   "outputs": [],
   "source": [
    "# TODO: generate digits using the decoder of VAE (1 point)\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b290a6",
   "metadata": {},
   "source": [
    "## Ablation Study: Beta-VAE (3 points)\n",
    "\n",
    "Now that you've trained a standard VAE, let's explore how the balance between reconstruction and KL divergence affects model performance. The VAE loss function is:\n",
    "\n",
    "$$\\mathcal{L} = \\text{Reconstruction Loss} + \\beta \\times \\text{KL Divergence}$$\n",
    "\n",
    "When $\\beta = 1$, this is the standard VAE. When $\\beta \\neq 1$, we have a **Beta-VAE**, which can lead to different properties:\n",
    "- **Lower Œ≤ (< 1)**: Prioritizes reconstruction quality over latent regularization\n",
    "- **Higher Œ≤ (> 1)**: Prioritizes latent regularization, potentially leading to better disentanglement\n",
    "\n",
    "**Your Task:** Train three VAE variants and compare them:\n",
    "1. **Beta-VAE with Œ≤ = 0.5** (prioritize reconstruction)\n",
    "2. **Standard VAE with Œ≤ = 1.0** (balanced - you may reuse your previous model)\n",
    "3. **Beta-VAE with Œ≤ = 2.0** (prioritize regularization)\n",
    "\n",
    "**Requirements:**\n",
    "- Train each variant for 10 epochs with the same hyperparameters (optimizer, learning rate, etc.)\n",
    "- Plot the **total loss curves** for all three models on the **same plot** with a legend\n",
    "- Generate **10 random samples** from each trained model (3 sets of 10 images total)\n",
    "- Show the generated images in a grid (you can use subplots: 3 rows √ó 10 columns, or separate grids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e62516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Beta-VAE ablation study\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "# Hint: You may want to modify the VAE class to accept a beta parameter\n",
    "# or create three separate training loops with different beta values\n",
    "\n",
    "# Train beta=0.5 model\n",
    "\n",
    "\n",
    "# Train beta=1.0 model (or reuse from previous section)\n",
    "\n",
    "\n",
    "# Train beta=2.0 model\n",
    "\n",
    "\n",
    "# Plot loss curves for all three models on the same plot\n",
    "\n",
    "\n",
    "# Generate and display 10 samples from each model\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80699e70",
   "metadata": {},
   "source": [
    "### Analysis and Comparison (Required)\n",
    "\n",
    "Based on your experiments above, answer the following questions:\n",
    "\n",
    "**1. Loss Curve Analysis:** \n",
    "Compare the loss curves of the three models. Which model converged to the lowest loss? Does lower loss always mean better performance? Explain.\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**2. Generated Image Quality:** \n",
    "Compare the quality and diversity of generated images across the three models. Which Œ≤ value produces the best-looking digits? Which produces the most diverse samples? Provide specific observations.\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**3. Trade-off Analysis:** \n",
    "Explain the trade-off between reconstruction quality and latent space regularization. Based on your results, which Œ≤ value would you recommend for this task and why?\n",
    "\n",
    "[Your answer here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd593806",
   "metadata": {
    "id": "dd593806"
   },
   "source": [
    "## Task 3: Try Vector Quantized Variational Autoencoder (VQ-VAE) (Bonus: 5 points)\n",
    "\n",
    "More realisticly, images can have more discrete categories. In this task, we will try to use Vector Quantized Variational Autoencoder (VQ-VAE) to generate images. VQ-VAE is a variant of VAE that uses a discrete latent space.\n",
    "\n",
    "With the backbone of VAE, VQ-VAE introduces a codebook $ùê∂$ to quantize the latent space. Specifically, instead of directly encoding a continuous latent variable $z$, VQ-VAE enforces that the latent representations are drawn from a finite set of discrete codes in the codebook $C$. During encoding, each latent vector is replaced by its nearest neighbor in the codebook, introducing quantization to the latent space. This process allows the model to learn a discrete, compact representation of the data.\n",
    "\n",
    "Please refer to this [link](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a) for more details.\n",
    "\n",
    "### Implementation tips\n",
    "\n",
    "1. **Codebook**: Use nn.Embedding to implement the codebook. The weights should be initialized with a uniform distribution.\n",
    "2. **Quantization**: Given the continuous latent representation $z_e$, compute distances $d^2 = z^2 + e^2 - 2 e * z$. Then, find the index of the nearest neighbor in the codebook. Use tensor.detach() to prevent gradients from flowing through the codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f57dce",
   "metadata": {
    "id": "57f57dce"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.codebook = # TODO: define the codebook\n",
    "        # TODO: initialize the codebook\n",
    "\n",
    "    def forward(self, z_f):\n",
    "        # TODO:\n",
    "        ## 1. Compute the distance between flattend latent z_f and the embedding vectors\n",
    "        ## 2. Find the closest encodings\n",
    "        ## 3. Get the quantized latent vectors\n",
    "        ##############################################\n",
    "        # Your code starts here\n",
    "        ##############################################\n",
    "\n",
    "\n",
    "        ##############################################\n",
    "        # Your code ends here\n",
    "        ##############################################\n",
    "\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc636f4",
   "metadata": {
    "id": "bcc636f4"
   },
   "source": [
    "The backbone of the VQ-VAE model has been implemented for you and hyperparameters are set. Please implement the VectorQuantizer and the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea533d",
   "metadata": {
    "id": "97ea533d"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_dim, res_h_dim, kernel_size=3,\n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1,\n",
    "                      stride=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)]*n_res_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the q_theta (z|x) network. Given a data sample x q_theta\n",
    "    maps to the latent space x -> z.\n",
    "\n",
    "    For a VQ VAE, q_theta outputs parameters of a categorical distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=kernel,\n",
    "                      stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=kernel,\n",
    "                      stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim, h_dim, kernel_size=kernel-1,\n",
    "                      stride=stride-1, padding=1),\n",
    "            ResidualStack(\n",
    "                h_dim, h_dim, res_h_dim, n_res_layers)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the p_phi (x|z) network. Given a latent sample z p_phi\n",
    "    maps back to the original space z -> x.\n",
    "\n",
    "    Inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - h_dim : the hidden layer dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(h_dim, h_dim // 2,\n",
    "                               kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(h_dim//2, 3, kernel_size=kernel,\n",
    "                               stride=stride, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, res_latent_dim, n_res_layers, embedding_dim, codebook_size, beta=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.codebook_size = codebook_size\n",
    "        self.beta = beta\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(3, latent_dim, n_res_layers, res_latent_dim)\n",
    "\n",
    "        # Vector quantization\n",
    "        self.pre_quantization_conv = nn.Conv2d(latent_dim, embedding_dim, kernel_size=1, stride=1)\n",
    "        self.vector_quantization = VectorQuantizer(\n",
    "            codebook_size, embedding_dim, beta)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(embedding_dim, latent_dim, n_res_layers, res_latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        z_e = self.encoder(x)\n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "\n",
    "        # Quantize\n",
    "        B, C, H, W = z_e.shape\n",
    "        z_e = z_e.permute(0, 2, 3, 1).contiguous()\n",
    "        z_f = z_e.view(-1, C)\n",
    "        z_q = self.vector_quantization(z_f)\n",
    "        z_q = z_q.view(B, H, W, -1)\n",
    "        codebook_loss = torch.mean((z_q.detach()-z_e)**2) + self.beta * \\\n",
    "            torch.mean((z_q - z_e.detach()) ** 2)\n",
    "        z_q = z_e + (z_q - z_e).detach()    # Prevent backpropagation to codebook\n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Decode\n",
    "        x_recon = self.decoder(z_q)\n",
    "\n",
    "        return x_recon, z_e, z_q, codebook_loss\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 3 * 32 * 32\n",
    "latent_dim = 128\n",
    "res_latent_dim = 32\n",
    "n_res_layers = 2\n",
    "embedding_dim = 64\n",
    "codebook_size = 512\n",
    "model = VQVAE(input_dim, latent_dim, res_latent_dim, n_res_layers, embedding_dim, codebook_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418df703",
   "metadata": {
    "id": "418df703"
   },
   "source": [
    "To demonstrate the effectiveness of VQ-VAE, we adopt CIFAR-10 dataset in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d63e93",
   "metadata": {
    "id": "30d63e93"
   },
   "outputs": [],
   "source": [
    "# Using CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8c408",
   "metadata": {
    "id": "9cd8c408"
   },
   "source": [
    "The training pipeline is similar to VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4703c30",
   "metadata": {
    "id": "f4703c30"
   },
   "outputs": [],
   "source": [
    "# TODO: train the VQ-VAE model\n",
    "##############################################\n",
    "# Your code starts here\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Your code ends here\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8f38b",
   "metadata": {
    "id": "64d8f38b"
   },
   "source": [
    "We can evaluate the model by checking the reconstruction loss and the generated images. Please run the code below to generate images for assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a79689",
   "metadata": {
    "id": "35a79689"
   },
   "outputs": [],
   "source": [
    "# Visualizing reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "sample_list = []\n",
    "batch = next(iter(test_loader))\n",
    "for i in range(10):\n",
    "    sample_list.append(batch[0][i])\n",
    "x_orig = torch.stack(sample_list).to(device)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon, mu, log_var, _ = model(x_orig)\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "imgs_orig = make_grid(x_orig, nrow=10, padding=0, normalize=True)\n",
    "imgs_recon = make_grid(x_recon, nrow=10, padding=0, normalize=True)\n",
    "\n",
    "print('Input Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_orig.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Reconstructed Images:')\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(imgs_recon.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
